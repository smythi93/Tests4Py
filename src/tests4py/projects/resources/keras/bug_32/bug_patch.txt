diff --git a/keras/callbacks.py b/keras/callbacks.py
index 8b5ec5f3..eb233de3 100644
--- a/keras/callbacks.py
+++ b/keras/callbacks.py
@@ -898,7 +898,7 @@ class ReduceLROnPlateau(Callback):
             monitored has stopped increasing; in `auto`
             mode, the direction is automatically inferred
             from the name of the monitored quantity.
-        epsilon: threshold for measuring the new optimum,
+        min_delta: threshold for measuring the new optimum,
             to only focus on significant changes.
         cooldown: number of epochs to wait before resuming
             normal operation after lr has been reduced.
@@ -906,16 +906,21 @@ class ReduceLROnPlateau(Callback):
     """
 
     def __init__(self, monitor='val_loss', factor=0.1, patience=10,
-                 verbose=0, mode='auto', epsilon=1e-4, cooldown=0, min_lr=0):
+                 verbose=0, mode='auto', min_delta=1e-4, cooldown=0, min_lr=0,
+                 **kwargs):
         super(ReduceLROnPlateau, self).__init__()
 
         self.monitor = monitor
         if factor >= 1.0:
             raise ValueError('ReduceLROnPlateau '
                              'does not support a factor >= 1.0.')
+        if 'epsilon' in kwargs:
+            min_delta = kwargs.pop('epsilon')
+            warnings.warn('`epsilon` argument is deprecated and '
+                          'will be removed, use `min_delta` insted.')
         self.factor = factor
         self.min_lr = min_lr
-        self.epsilon = epsilon
+        self.min_delta = min_delta
         self.patience = patience
         self.verbose = verbose
         self.cooldown = cooldown
@@ -936,10 +941,10 @@ class ReduceLROnPlateau(Callback):
             self.mode = 'auto'
         if (self.mode == 'min' or
            (self.mode == 'auto' and 'acc' not in self.monitor)):
-            self.monitor_op = lambda a, b: np.less(a, b - self.epsilon)
+            self.monitor_op = lambda a, b: np.less(a, b - self.min_delta)
             self.best = np.Inf
         else:
-            self.monitor_op = lambda a, b: np.greater(a, b + self.epsilon)
+            self.monitor_op = lambda a, b: np.greater(a, b + self.min_delta)
             self.best = -np.Inf
         self.cooldown_counter = 0
         self.wait = 0
@@ -967,6 +972,7 @@ class ReduceLROnPlateau(Callback):
                 self.best = current
                 self.wait = 0
             elif not self.in_cooldown():
+                self.wait += 1
                 if self.wait >= self.patience:
                     old_lr = float(K.get_value(self.model.optimizer.lr))
                     if old_lr > self.min_lr:
@@ -978,7 +984,6 @@ class ReduceLROnPlateau(Callback):
                                   'rate to %s.' % (epoch + 1, new_lr))
                         self.cooldown_counter = self.cooldown
                         self.wait = 0
-                self.wait += 1
 
     def in_cooldown(self):
         return self.cooldown_counter > 0
