diff --git a/keras/layers/recurrent.py b/keras/layers/recurrent.py
index f31286db..81c367f9 100644
--- a/keras/layers/recurrent.py
+++ b/keras/layers/recurrent.py
@@ -518,12 +518,14 @@ class RNN(Layer):
             self._num_constants = len(constants)
             additional_specs += self.constants_spec
         # at this point additional_inputs cannot be empty
-        is_keras_tensor = hasattr(additional_inputs[0], '_keras_history')
+        is_keras_tensor = K.is_keras_tensor(additional_inputs[0])
         for tensor in additional_inputs:
-            if hasattr(tensor, '_keras_history') != is_keras_tensor:
+            if K.is_keras_tensor(tensor) != is_keras_tensor:
                 raise ValueError('The initial state or constants of an RNN'
                                  ' layer cannot be specified with a mix of'
-                                 ' Keras tensors and non-Keras tensors')
+                                 ' Keras tensors and non-Keras tensors'
+                                 ' (a "Keras tensor" is a tensor that was'
+                                 ' returned by a Keras layer, or by `Input`)')
 
         if is_keras_tensor:
             # Compute the full input spec, including state and constants
diff --git a/keras/layers/wrappers.py b/keras/layers/wrappers.py
index 7febff35..a5eed289 100644
--- a/keras/layers/wrappers.py
+++ b/keras/layers/wrappers.py
@@ -275,6 +275,7 @@ class Bidirectional(Wrapper):
         self.supports_masking = True
         self._trainable = True
         super(Bidirectional, self).__init__(layer, **kwargs)
+        self.input_spec = layer.input_spec
 
     @property
     def trainable(self):
@@ -313,6 +314,60 @@ class Bidirectional(Wrapper):
             return [output_shape] + state_shape + copy.copy(state_shape)
         return output_shape
 
+    def __call__(self, inputs, initial_state=None, **kwargs):
+        if isinstance(inputs, list):
+            if len(inputs) > 1:
+                initial_state = inputs[1:]
+            inputs = inputs[0]
+
+        if initial_state is None:
+            return super(Bidirectional, self).__call__(inputs, **kwargs)
+
+        # Standardize `initial_state` into list
+        if isinstance(initial_state, tuple):
+            initial_state = list(initial_state)
+        elif not isinstance(initial_state, list):
+            initial_state = [initial_state]
+
+        # Check if `initial_state` can be splitted into half
+        num_states = len(initial_state)
+        if num_states % 2 > 0:
+            raise ValueError(
+                'When passing `initial_state` to a Bidirectional RNN, the state '
+                'should be a list containing the states of the underlying RNNs. '
+                'Found: ' + str(initial_state))
+
+        # Applies the same workaround as in `RNN.__call__`, without handling constants
+        kwargs['initial_state'] = initial_state
+        additional_inputs = initial_state
+        additional_specs = [InputSpec(shape=K.int_shape(state))
+                            for state in initial_state]
+        self.forward_layer.state_spec = additional_specs[:num_states // 2]
+        self.backward_layer.state_spec = additional_specs[num_states // 2:]
+
+        is_keras_tensor = K.is_keras_tensor(additional_inputs[0])
+        for tensor in additional_inputs:
+            if K.is_keras_tensor(tensor) != is_keras_tensor:
+                raise ValueError('The initial state of a Bidirectional'
+                                 ' layer cannot be specified with a mix of'
+                                 ' Keras tensors and non-Keras tensors'
+                                 ' (a "Keras tensor" is a tensor that was'
+                                 ' returned by a Keras layer, or by `Input`)')
+
+        if is_keras_tensor:
+            # Compute the full input spec, including state
+            full_input = [inputs] + additional_inputs
+            full_input_spec = self.input_spec + additional_specs
+
+            # Perform the call with temporarily replaced input_spec
+            original_input_spec = self.input_spec
+            self.input_spec = full_input_spec
+            output = super(Bidirectional, self).__call__(full_input, **kwargs)
+            self.input_spec = original_input_spec
+            return output
+        else:
+            return super(Bidirectional, self).__call__(inputs, **kwargs)
+
     def call(self, inputs, training=None, mask=None, initial_state=None):
         kwargs = {}
         if has_arg(self.layer.call, 'training'):
@@ -321,11 +376,6 @@ class Bidirectional(Wrapper):
             kwargs['mask'] = mask
 
         if initial_state is not None and has_arg(self.layer.call, 'initial_state'):
-            if not isinstance(initial_state, list):
-                raise ValueError(
-                    'When passing `initial_state` to a Bidirectional RNN, the state '
-                    'should be a list containing the states of the underlying RNNs. '
-                    'Found: ' + str(initial_state))
             forward_state = initial_state[:len(initial_state) // 2]
             backward_state = initial_state[len(initial_state) // 2:]
             y = self.forward_layer.call(inputs, initial_state=forward_state, **kwargs)
